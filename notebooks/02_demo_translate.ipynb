{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 02 · Demo Translate (EN→VI)\n\nNotebook này nạp checkpoint từ `models/checkpoints/tiny_transformer_en2vi.pt` và cho phép bạn nhập **câu tiếng Anh** để suy diễn bản dịch **tiếng Việt**."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import json, math, pathlib, torch, torch.nn as nn, torch.nn.functional as F\nfrom pathlib import Path\n\nPROJECT_ROOT = pathlib.Path(__file__).resolve().parents[1] if '__file__' in globals() else pathlib.Path.cwd().parents[1]\nCKPT = PROJECT_ROOT / \"models\" / \"checkpoints\" / \"tiny_transformer_en2vi.pt\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\nckpt = torch.load(CKPT, map_location=device)\nsrc2i, i2src = ckpt[\"src2i\"], ckpt[\"i2src\"]\ntgt2i, i2tgt = ckpt[\"tgt2i\"], ckpt[\"i2tgt\"]\nPAD, BOS, EOS, UNK = 0,1,2,3\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Định nghĩa lại kiến trúc tối giản (khớp với train nb)\nimport math, torch, torch.nn as nn, torch.nn.functional as F\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\ndef attention(q, k, v, mask=None, dropout=None):\n    d_k = q.size(-1)\n    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask==0, float(\"-inf\"))\n    attn = torch.softmax(scores, dim=-1)\n    if dropout is not None:\n        attn = dropout(attn)\n    return torch.matmul(attn, v), attn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0\n        self.d_k = d_model // h\n        self.h = h\n        self.linear_q = nn.Linear(d_model, d_model)\n        self.linear_k = nn.Linear(d_model, d_model)\n        self.linear_v = nn.Linear(d_model, d_model)\n        self.linear_out = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, query, key, value, mask=None):\n        bs = query.size(0)\n        def split(x):\n            return x.view(bs, -1, self.h, self.d_k).transpose(1,2)\n        q = split(self.linear_q(query))\n        k = split(self.linear_k(key))\n        v = split(self.linear_v(value))\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n        x, _ = attention(q, k, v, mask=mask, dropout=self.dropout)\n        x = x.transpose(1,2).contiguous().view(bs, -1, self.h*self.d_k)\n        return self.linear_out(x)\n\nclass PositionwiseFFN(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.w1 = nn.Linear(d_model, d_ff)\n        self.w2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        return self.w2(self.dropout(F.relu(self.w1(x))))\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, self_h, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(self_h, d_model, dropout)\n        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.drop = nn.Dropout(dropout)\n    def forward(self, x, src_mask):\n        x2 = self.self_attn(x, x, x, src_mask)\n        x = self.norm1(x + self.drop(x2))\n        x2 = self.ffn(x)\n        x = self.norm2(x + self.drop(x2))\n        return x\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, self_h, cross_h, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(self_h, d_model, dropout)\n        self.cross_attn = MultiHeadAttention(cross_h, d_model, dropout)\n        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.drop = nn.Dropout(dropout)\n    def forward(self, x, memory, tgt_mask, src_mask):\n        x2 = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.drop(x2))\n        x2 = self.cross_attn(x, memory, memory, src_mask)\n        x = self.norm2(x + self.drop(x2))\n        x2 = self.ffn(x)\n        x = self.norm3(x + self.drop(x2))\n        return x\n\ndef subsequent_mask(sz):\n    import torch\n    attn_shape = (1, sz, sz)\n    subsequent = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n    return (subsequent == 0)\n\nclass TinyTransformer(nn.Module):\n    def __init__(self, src_vocab, tgt_vocab, d_model=128, N=2, h=4, d_ff=256, dropout=0.1):\n        super().__init__()\n        self.src_embed = nn.Embedding(src_vocab, d_model, padding_idx=PAD)\n        self.tgt_embed = nn.Embedding(tgt_vocab, d_model, padding_idx=PAD)\n        self.pos = PositionalEncoding(d_model)\n        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, h, d_ff, dropout) for _ in range(N)])\n        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, h, h, d_ff, dropout) for _ in range(N)])\n        self.proj = nn.Linear(d_model, tgt_vocab)\n    def encode(self, src, src_mask):\n        x = self.pos(self.src_embed(src))\n        for layer in self.enc_layers:\n            x = layer(x, src_mask)\n        return x\n    def decode(self, tgt, memory, tgt_mask, src_mask):\n        x = self.pos(self.tgt_embed(tgt))\n        for layer in self.dec_layers:\n            x = layer(x, memory, tgt_mask, src_mask)\n        return x\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        memory = self.encode(src, src_mask)\n        dec = self.decode(tgt, memory, tgt_mask, src_mask)\n        return self.proj(dec)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def tokenize_ws(s): \n    return s.lower().strip().split()\n\ndef encode_line(s, stoi, add_bos=False, add_eos=True, max_len=64):\n    ids = [stoi.get(t, 3) for t in tokenize_ws(s)]  # UNK=3\n    if add_bos: ids = [1] + ids\n    if add_eos: ids = ids + [2]\n    return ids[:max_len]\n\ndef make_src_mask(src): return (src != 0).unsqueeze(1).unsqueeze(2)\ndef make_tgt_mask(tgt):\n    b, t = tgt.size()\n    pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n    sub_mask = subsequent_mask(t).to(tgt.device)\n    return pad_mask & sub_mask\n\ndef greedy_decode(model, src_ids, max_len=64):\n    model.eval()\n    src = torch.tensor([src_ids], dtype=torch.long, device=device)\n    src_mask = make_src_mask(src)\n    memory = model.encode(src, src_mask)\n    ys = torch.tensor([[1]], dtype=torch.long, device=device) # BOS\n    for _ in range(max_len-1):\n        tgt_mask = make_tgt_mask(ys)\n        out = model.decode(ys, memory, tgt_mask, src_mask)\n        prob = model.proj(out)[:, -1, :].softmax(-1)\n        next_token = prob.argmax(-1).item()\n        ys = torch.cat([ys, torch.tensor([[next_token]], device=device)], dim=1)\n        if next_token == 2: break  # EOS\n    return ys.squeeze(0).tolist()\n\ndef detok(ids, i2w):\n    toks = []\n    for i in ids:\n        if i in (0,1): continue\n        if i == 2: break\n        toks.append(i2w[i])\n    return \" \".join(toks)\n\n# Khởi tạo & load state\nmodel = TinyTransformer(len(i2src), len(i2tgt)).to(device)\nmodel.load_state_dict(ckpt[\"model\"], strict=True)\nmodel.eval()\nprint(\"Checkpoint loaded from\", CKPT)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ✨ Gõ câu tiếng Anh để dịch\ntext_en = \"This is a small dataset.\"\nsrc_ids = encode_line(text_en, src2i, add_bos=False, add_eos=True)\npred_ids = greedy_decode(model, src_ids, max_len=64)\nprint(\"EN:\", text_en)\nprint(\"VI:\", detok(pred_ids, i2tgt))\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}